{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ded731d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2eec7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate # pip install evaluate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b587200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir='./trained_model', num_train_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0206e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/epdls/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1afab75798441dd80af39afee6c0b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\epdls\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-16ad00c953871f0f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\epdls\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-9690c23dbc2848e5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\epdls\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-372ad965bbe7ba4b.arrow\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856ce49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1811cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['validation'],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61d4646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\epdls\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4590' max='4590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4590/4590 04:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.007700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./trained_model\\checkpoint-500\n",
      "Configuration saved in ./trained_model\\checkpoint-500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-1000\n",
      "Configuration saved in ./trained_model\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-1500\n",
      "Configuration saved in ./trained_model\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-2000\n",
      "Configuration saved in ./trained_model\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-2000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-2500\n",
      "Configuration saved in ./trained_model\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-2500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-3000\n",
      "Configuration saved in ./trained_model\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-3500\n",
      "Configuration saved in ./trained_model\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-3500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-4000\n",
      "Configuration saved in ./trained_model\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-4000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-4500\n",
      "Configuration saved in ./trained_model\\checkpoint-4500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-4500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4590, training_loss=0.1536856976708028, metrics={'train_runtime': 270.8374, 'train_samples_per_second': 135.432, 'train_steps_per_second': 16.947, 'total_flos': 1353749546484720.0, 'train_loss': 0.1536856976708028, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "471ebe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(tokenized_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60b2c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8455882352941176, 'f1': 0.891566265060241}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "metric = evaluate.load('glue', 'mrpc')\n",
    "metric.compute(predictions=preds, references=pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7209c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c459a6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='./trained_model', num_train_epochs=10, evaluation_strategy='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664a8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e762e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\epdls\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4590' max='4590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4590/4590 04:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.055631</td>\n",
       "      <td>0.821078</td>\n",
       "      <td>0.874786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>0.828091</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.898799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>1.056531</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>0.891566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>1.101945</td>\n",
       "      <td>0.825980</td>\n",
       "      <td>0.877797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>1.153370</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.893471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>1.358433</td>\n",
       "      <td>0.825980</td>\n",
       "      <td>0.873890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.384753</td>\n",
       "      <td>0.835784</td>\n",
       "      <td>0.885470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>1.448529</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.887755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>1.541422</td>\n",
       "      <td>0.835784</td>\n",
       "      <td>0.886633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.584729</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.885135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-500\n",
      "Configuration saved in ./trained_model\\checkpoint-500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-1000\n",
      "Configuration saved in ./trained_model\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-1000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-1500\n",
      "Configuration saved in ./trained_model\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-1500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-2000\n",
      "Configuration saved in ./trained_model\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-2000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-2500\n",
      "Configuration saved in ./trained_model\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-2500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-3000\n",
      "Configuration saved in ./trained_model\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-3000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-3500\n",
      "Configuration saved in ./trained_model\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-3500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-4000\n",
      "Configuration saved in ./trained_model\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-4000\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-4500\n",
      "Configuration saved in ./trained_model\\checkpoint-4500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-4500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4590, training_loss=0.034586332761428945, metrics={'train_runtime': 299.037, 'train_samples_per_second': 122.66, 'train_steps_per_second': 15.349, 'total_flos': 1353749546484720.0, 'train_loss': 0.034586332761428945, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c09547ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 01:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(tokenized_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3f423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 408\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5847285985946655,\n",
       " 'eval_accuracy': 0.8333333333333334,\n",
       " 'eval_f1': 0.8851351351351352,\n",
       " 'eval_runtime': 3.1198,\n",
       " 'eval_samples_per_second': 130.776,\n",
       " 'eval_steps_per_second': 16.347,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f08975",
   "metadata": {},
   "source": [
    "✏️ **Try it out!** Fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2474a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/epdls/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb0dbdc2cc242ba98ee9fa109889a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\epdls/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\epdls/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\epdls/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\epdls/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\epdls/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c29b4f3e0c48abbb1d62d440188a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\epdls\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-9813c7bd64b24f69.arrow\n",
      "Loading cached processed dataset at C:\\Users\\epdls\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-aa1f46b739d75720.arrow\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed4ecf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='./trained_model', num_train_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "362f3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    per_device_train_batch_size=32,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "105bfe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\epdls\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 67349\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 84190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25259' max='84190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25259/84190 24:23 < 56:55, 17.26 it/s, Epoch 3.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.347800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.294200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.294100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.250300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.255800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.302200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./trained_model\\checkpoint-500\n",
      "Configuration saved in ./trained_model\\checkpoint-500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-1000\n",
      "Configuration saved in ./trained_model\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-1500\n",
      "Configuration saved in ./trained_model\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-2000\n",
      "Configuration saved in ./trained_model\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-2000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-2500\n",
      "Configuration saved in ./trained_model\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-2500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-3000\n",
      "Configuration saved in ./trained_model\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-3500\n",
      "Configuration saved in ./trained_model\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-3500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-4000\n",
      "Configuration saved in ./trained_model\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-4000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-4500\n",
      "Configuration saved in ./trained_model\\checkpoint-4500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-4500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-5000\n",
      "Configuration saved in ./trained_model\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-5500\n",
      "Configuration saved in ./trained_model\\checkpoint-5500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-5500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-6000\n",
      "Configuration saved in ./trained_model\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-6000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-6500\n",
      "Configuration saved in ./trained_model\\checkpoint-6500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-6500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-7000\n",
      "Configuration saved in ./trained_model\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-7000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-7500\n",
      "Configuration saved in ./trained_model\\checkpoint-7500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-7500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-8000\n",
      "Configuration saved in ./trained_model\\checkpoint-8000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-8000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-8500\n",
      "Configuration saved in ./trained_model\\checkpoint-8500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-8500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-9000\n",
      "Configuration saved in ./trained_model\\checkpoint-9000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-9000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-9500\n",
      "Configuration saved in ./trained_model\\checkpoint-9500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-9500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-10000\n",
      "Configuration saved in ./trained_model\\checkpoint-10000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-10500\n",
      "Configuration saved in ./trained_model\\checkpoint-10500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-10500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-11000\n",
      "Configuration saved in ./trained_model\\checkpoint-11000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-11000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-11500\n",
      "Configuration saved in ./trained_model\\checkpoint-11500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./trained_model\\checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-11500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-12000\n",
      "Configuration saved in ./trained_model\\checkpoint-12000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-12000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-12500\n",
      "Configuration saved in ./trained_model\\checkpoint-12500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-12500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-12500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-12500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-13000\n",
      "Configuration saved in ./trained_model\\checkpoint-13000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-13000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-13500\n",
      "Configuration saved in ./trained_model\\checkpoint-13500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-13500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-13500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-13500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-14000\n",
      "Configuration saved in ./trained_model\\checkpoint-14000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-14000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-14500\n",
      "Configuration saved in ./trained_model\\checkpoint-14500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-14500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-14500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-14500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-15000\n",
      "Configuration saved in ./trained_model\\checkpoint-15000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-15500\n",
      "Configuration saved in ./trained_model\\checkpoint-15500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-15500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-15500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-15500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-16000\n",
      "Configuration saved in ./trained_model\\checkpoint-16000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-16000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-16500\n",
      "Configuration saved in ./trained_model\\checkpoint-16500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-16500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-16500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-16500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-17000\n",
      "Configuration saved in ./trained_model\\checkpoint-17000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-17000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-17500\n",
      "Configuration saved in ./trained_model\\checkpoint-17500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-17500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-17500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-17500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-18000\n",
      "Configuration saved in ./trained_model\\checkpoint-18000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-18000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-18500\n",
      "Configuration saved in ./trained_model\\checkpoint-18500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-18500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-18500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-18500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-19000\n",
      "Configuration saved in ./trained_model\\checkpoint-19000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-19000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-19000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-19000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-19500\n",
      "Configuration saved in ./trained_model\\checkpoint-19500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-19500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-19500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-19500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-20000\n",
      "Configuration saved in ./trained_model\\checkpoint-20000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-20000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-20500\n",
      "Configuration saved in ./trained_model\\checkpoint-20500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-20500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-20500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-20500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-21000\n",
      "Configuration saved in ./trained_model\\checkpoint-21000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-21000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-21000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-21000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-21500\n",
      "Configuration saved in ./trained_model\\checkpoint-21500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-21500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-21500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-21500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-22000\n",
      "Configuration saved in ./trained_model\\checkpoint-22000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-22000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-22000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-22000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-22500\n",
      "Configuration saved in ./trained_model\\checkpoint-22500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./trained_model\\checkpoint-22500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-22500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-22500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-23000\n",
      "Configuration saved in ./trained_model\\checkpoint-23000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-23000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-23000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-23000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-23500\n",
      "Configuration saved in ./trained_model\\checkpoint-23500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-23500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-23500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-23500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-24000\n",
      "Configuration saved in ./trained_model\\checkpoint-24000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-24000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-24500\n",
      "Configuration saved in ./trained_model\\checkpoint-24500\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-24500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-24500\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-24500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./trained_model\\checkpoint-25000\n",
      "Configuration saved in ./trained_model\\checkpoint-25000\\config.json\n",
      "Model weights saved in ./trained_model\\checkpoint-25000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./trained_model\\checkpoint-25000\\tokenizer_config.json\n",
      "Special tokens file saved in ./trained_model\\checkpoint-25000\\special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1407\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         )\n\u001b[1;32m-> 1409\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1410\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1716\u001b[0m                         \u001b[0moptimizer_was_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscale_before\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mscale_after\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1717\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1718\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1720\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0moptimizer_was_run\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    358\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m                 \u001b[1;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd8c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
